version: '3.8'

services:
  # PostgreSQL for metadata storage
  postgres:
    image: postgres:15-alpine
    container_name: dq_postgres
    environment:
      POSTGRES_USER: dquser
      POSTGRES_PASSWORD: dqpass
      POSTGRES_DB: data_quality
      POSTGRES_MULTIPLE_DATABASES: data_quality,airflow,mlflow
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/init-databases.sh:/docker-entrypoint-initdb.d/init-databases.sh
    networks:
      - dq_network

  # Redis for real-time alerting
  redis:
    image: redis:7-alpine
    container_name: dq_redis
    ports:
      - "6379:6379"
    networks:
      - dq_network

  # Zookeeper for Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: dq_zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - dq_network

  # Kafka broker
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: dq_kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks:
      - dq_network

  # MLflow tracking server
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.9.2
    container_name: dq_mlflow
    ports:
      - "5001:5000"
    environment:
      MLFLOW_BACKEND_STORE_URI: postgresql://dquser:dqpass@postgres:5432/mlflow
      MLFLOW_DEFAULT_ARTIFACT_ROOT: /mlflow/artifacts
    volumes:
      - mlflow_data:/mlflow
    depends_on:
      - postgres
    command: >
      mlflow server --backend-store-uri postgresql://dquser:dqpass@postgres:5432/mlflow --default-artifact-root /mlflow/artifacts --host 0.0.0.0 --port 5000
    networks:
      - dq_network

  # Prometheus for monitoring
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: dq_prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./configs/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    networks:
      - dq_network

  # Grafana for dashboards
  grafana:
    image: grafana/grafana:10.2.2
    container_name: dq_grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_INSTALL_PLUGINS: grafana-piechart-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ./configs/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./configs/grafana/datasources:/etc/grafana/provisioning/datasources
    depends_on:
      - prometheus
      - postgres
    networks:
      - dq_network

  # Airflow init (one-time database setup)
  airflow-init:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
    container_name: dq_airflow_init
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://dquser:dqpass@postgres:5432/airflow
    command:
      - bash
      - -c
      - |
        airflow db migrate
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true
    networks:
      - dq_network

  # Airflow webserver
  airflow:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
    container_name: dq_airflow
    depends_on:
      - postgres
      - redis
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://dquser:dqpass@postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__WEBSERVER__SECRET_KEY: "dq_secret_key_12345"
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
    ports:
      - "8080:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - ./data:/opt/airflow/data
      - airflow_logs:/opt/airflow/logs
    command:
      - bash
      - -c
      - |
        airflow webserver &
        airflow scheduler
    networks:
      - dq_network

  # Data Quality API Service
  quality_api:
    build:
      context: .
      dockerfile: docker/Dockerfile.api
    container_name: dq_api
    depends_on:
      - postgres
      - redis
      - kafka
    ports:
      - "8000:8000"
    environment:
      DATABASE_URL: postgresql://dquser:dqpass@postgres:5432/data_quality
      REDIS_URL: redis://redis:6379
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
    volumes:
      - ./src:/app/src
      - ./models:/app/models
      - ./configs:/app/configs
    command: uvicorn src.api.quality_service:app --host 0.0.0.0 --port 8000 --reload
    networks:
      - dq_network

volumes:
  postgres_data:
  redis_data:
  mlflow_data:
  prometheus_data:
  grafana_data:
  airflow_logs:


networks:
  dq_network:
    driver: bridge
